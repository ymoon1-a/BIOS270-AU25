Examine create_bacteria_db.sh, how many tables will be created in the database?
The script create_bacteria_db.sh calls three Python scripts:
insert_gff_table.py
insert_protein_cluster_table.py
insert_metadata_table.py
Each of these scripts creates (or appends to) a table in the SQLite database.
Therefore, the database will contain 3 tables.

In the insert_gff_table.py script you submitted, explain the logic of using try and except. Why is this necessary?
The try/except block is designed to handle the situation where multiple Slurm array jobs attempt to write to the SQLite database at the same time.
SQLite allows only one writer at a time.
If two or more jobs attempt to insert into the database simultaneously, SQLite will raise:
sqlite3.OperationalError: database is locked

This happens frequently because create_bacteria_db.sh is run as a Slurm job array (0–4), meaning five parallel tasks are all trying to write to the same database.

Complete the TODO sections in query_bacteria_db.py. You may want to examine the gff2df function in insert_gff_table.py to understand the columns in gff table.
After completing the sections, this is what I got:

Total number of record ids:  4100
Processed 0 record ids in 3.3719565868377686 seconds
Processed 10 record ids in 11.456568241119385 seconds
Processed 20 record ids in 19.585670471191406 seconds
Processed 30 record ids in 27.71912455558777 seconds
Processed 40 record ids in 35.85742378234863 seconds
Processed 50 record ids in 43.9911425113678 seconds
Processed 60 record ids in 52.13821601867676 seconds
Processed 70 record ids in 60.280190229415894 seconds

Uncommenting db.index_record_ids() creates an index on the record_id column in the gff table. 
Without the index, every query of the form SELECT … WHERE record_id = ? requires a full table scan, 
which is slow because the gff table contains many rows. With the index, SQLite can look up matching rows efficiently 
using the B-tree index structure, avoiding repeated full table scans. 
This reduces the lookup time from O(n) to O(log n) and significantly speeds up the entire script, 
especially because it performs thousands of such queries.


Explain the role of CHUNK_SIZE and why it is necessary:
CHUNK_SIZE limits how many rows are read from SQLite and uploaded to BigQuery at a time. 
This prevents memory overflow, reduces load job failures, keeps uploads stable, and makes it possible to process 
very large tables that would not fit in memory if loaded all at once.


Explain why the following chunk configuration makes sense - what kind of data access pattern is expected, and 
why does this align with biological use cases?

Each protein has two embedding vectors:
mean (length = 164)
mean_mid (length = 164)
So the datasets are shaped like:
(n_samples, 164)
where n_samples ≈ number of proteins (~2M across ~2000 genomes).
This is large, so data must be chunked.

The chunk size is set to 1000 because protein embeddings are typically accessed in batches rather than individually. 
Biological analyses (such as examining a whole bacterial genome, clustering proteins, or running ML algorithms) usually 
involve loading hundreds to thousands of proteins at a time. HDF5 loads entire chunks during reads, so using chunks of 1000 rows 
(≈650KB) greatly reduces the number of I/O operations and makes sequential or batch access efficient. 
Smaller chunks would cause excessive overhead, while larger chunks would be too large to cache and slow to read. 
Thus, (1000, n_features) is a good balance between memory efficiency and typical biological data access patterns.

